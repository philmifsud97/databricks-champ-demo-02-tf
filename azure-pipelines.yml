# Grab variables from the specific variable group and
# determine sourceBranchName (avoids SourchBranchName=merge
# for PR)
variables:
  - group: 'DLT Files In Repos Testing'
  - name: 'branchName'
    ${{ if startsWith(variables['Build.SourceBranch'], 'refs/heads/') }}:
      value: $[ replace(variables['Build.SourceBranch'], 'refs/heads/', '') ]
    ${{ if startsWith(variables['Build.SourceBranch'], 'refs/pull/') }}:
      value: $[ replace(variables['System.PullRequest.SourceBranch'], 'refs/heads/', '') ]

trigger:
  batch: true
  branches:
    include:
    - '*'
  paths:
    exclude:
      - README.md
      - LICENSE
      - images
      - terraform
      - .github

#  tags:
#    include:
#      - v*.*
#      - prod

# This need an additional debugging
# pr:
#   branches:
#     include:
#       - master
#       - releases
#   paths:
#     exclude:
#       - README.md
#       - images
      
stages:
- stage: onPush
  condition: |
    and(
      ne(variables['Build.SourceBranch'], 'refs/heads/releases'),
      not(startsWith(variables['Build.SourceBranch'], 'refs/tags/v'))
    )
  jobs:
  - job: onPushJob
    pool:
      vmImage: 'ubuntu-20.04'

    steps:
    - task: UsePythonVersion@0
      displayName: 'Use Python 3.8'
      inputs:
        versionSpec: 3.8

    - checkout: self
      displayName: 'Checkout & Build.Reason: $(Build.Reason) & Build.SourceBranchName: $(Build.SourceBranchName)'

    - script: |
        curl -sSL https://install.python-poetry.org | python -
        export PATH=$PATH:$HOME/.poetry/bin
        poetry install --no-root
      displayName: 'Install dependencies'

    - script: echo "##vso[task.prependpath]$HOME/.poetry/bin"
      displayName: Add poetry to PATH
    
# https://docs.databricks.com/dev-tools/api/latest/repos.html.
# this is simplification, and won't work with concurrent commits. Ideally there should be a separate repository for each commit
    - script: |
        echo "Checking out the $(branchName) branch"
        poetry run databricks repos update --path $(STAGING_DIRECTORY) --branch "$(branchName)"
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)
      displayName: 'Update Staging project'

    - script: |
        poetry run pytest tests/unit-local --junit-xml=test-local.xml --cov
      displayName: 'Execute local tests'
      
    - script: |
        poetry run nutter run "$(STAGING_DIRECTORY)/tests/unit-notebooks/" --cluster_id $(CLUSTER_ID) --recursive --junit_report --timeout 500
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)
      displayName: 'Execute Nutter tests'

    - task: PublishTestResults@2
      condition: succeededOrFailed()
      inputs:
        testResultsFormat: 'JUnit'
        testResultsFiles: '**/test-*.xml' 
        failTaskOnFailedTests: true

# Separate pipeline for releases branch
# Right now it's similar to the onPush stage, but runs the DLT integration test in addition to the local tests.
- stage: onRelease
  condition: |
    eq(variables['Build.SourceBranch'], 'refs/heads/releases')
  jobs:
  - job: onReleaseJob
    pool:
      vmImage: 'ubuntu-20.04'

    steps:
    - task: UsePythonVersion@0
      displayName: 'Use Python 3.8'
      inputs:
        versionSpec: 3.8

    - checkout: self
      displayName: 'Checkout & Build.Reason: $(Build.Reason) & Build.SourceBranchName: $(Build.SourceBranchName)'

    - script: |
        curl -sSL https://install.python-poetry.org | python -
        export PATH=$PATH:$HOME/.poetry/bin
        poetry install --no-root
      displayName: 'Install dependencies'

    - script: echo "##vso[task.prependpath]$HOME/.poetry/bin"
      displayName: Add poetry to PATH
    
# https://docs.databricks.com/dev-tools/api/latest/repos.html.
# this is simplification, and won't work with concurrent commits. Ideally there should be a separate repository for each commit
    - script: |
        echo "Checking out the $(branchName) branch"
        poetry run databricks repos update --path $(STAGING_DIRECTORY) --branch "$(branchName)"
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)
      displayName: 'Update Staging project'

    - script: |
        poetry run pytest tests/unit-local --junit-xml=test-local.xml --cov
      displayName: 'Execute local tests'
      
    - script: |
        poetry run nutter run "$(STAGING_DIRECTORY)/tests/unit-notebooks/" --cluster_id $(CLUSTER_ID) --recursive --junit_report --timeout 500
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)
      displayName: 'Execute Nutter tests'

    - script: |
        DLT_START=$SECONDS
        DLT_START_ISO=$(date --iso-8601=seconds)
        poetry run databricks pipelines start --pipeline-id $(TEST_DLT_PIPELINE_ID) --full-refresh
        sleep 15
        while true; do
          DLT_STATUS=$(poetry run databricks pipelines get --pipeline-id $(TEST_DLT_PIPELINE_ID) |jq -r '.latest_updates[0].state')
          if [ "$DLT_STATUS" = "COMPLETED" -o "$DLT_STATUS" = "CANCELED" -o "$DLT_STATUS" = "FAILED" ]; then
            echo "Exiting loop with status '$DLT_STATUS'"
            break
          fi
          echo "DLT pipeline status is '$DLT_STATUS'. Waiting..."
          sleep 15
        done
        DLT_FINISH=$SECONDS
        DLT_ERRORS=$(( "$DLT_STATUS" = "FAILED" ? 1 : 0 ))
        DLT_SKIPPED=$(( "$DLT_STATUS" = "CANCELED" ? 1 : 0 ))
        echo "<?xml version=\"1.0\" encoding=\"UTF-8\"?><testsuites><testsuite name=\"DLT Integration test\" tests=\"1\" skipped=\"$DLT_SKIPPED\" errors=\"$DLT_ERRORS\" failures=\"$DLT_ERRORS\" time=\"$((DLT_FINISH-DLT_START))\" timestamp=\"${DLT_START_ISO}\">" > test-dlt.xml
        echo "<testcase classname=\"DLTIntegration\" name=\"${TEST_DLT_PIPELINE_NAME}\" time=\"$((DLT_FINISH-DLT_START))\">" >> test-dlt.xml
        if [ "$DLT_STATUS" = "FAILED" ]; then
          DLT_UPDATE_ID=$(poetry run databricks pipelines get --pipeline-id $(TEST_DLT_PIPELINE_ID) |jq -r '.latest_updates[0].update_id')
          echo "<failure message=\"DLT test failure\">Pipeline update with ID ${DLT_UPDATE_ID} has failed</failure>" >> test-dlt.xml
        elif [ "$DLT_STATUS" = "CANCELED" ]; then
          echo '<skipped />' >> test-dlt.xml
        fi
        echo '</testcase></testsuite></testsuites>' >> test-dlt.xml
        if [ "$DLT_STATUS" != "COMPLETED" ]; then
          exit 1
        fi
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)
      displayName: 'Execute DLT Integration Test pipeline'
      
    - task: PublishTestResults@2
      condition: succeededOrFailed()
      inputs:
        testResultsFormat: 'JUnit'
        testResultsFiles: '**/test-*.xml' 
        failTaskOnFailedTests: true
